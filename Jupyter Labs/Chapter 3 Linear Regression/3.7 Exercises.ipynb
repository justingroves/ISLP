{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the null hypotheses to which the $p$-values given in Table 3.4 correspond. Explain what conclusions you can draw vased on these $p$-values. Your explanation should be phrased in terms of `sales`, `TV`, `radio`, and `newspaper`, rather than in terms of the coefficients of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Note Table 3.4: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\begin{array}{l|rrrr}\n",
    "        \\hline \n",
    "         & \\text{Coefficient} & \\text{Std. error} & t\\text{-statistic} & p\\text{-value} \\\\\n",
    "        \\hline\n",
    "        \\verb|Intercept| & 2.939 & 0.3119 & 9.42 & \\lt 0.0001 \\\\\n",
    "        \\verb|TV| & 0.046 & 0.0014 & 32.81 & \\lt 0.0001 \\\\\n",
    "        \\verb|radio| & 0.189 & 0.0086 & 21.89 & \\lt 0.0001 \\\\\n",
    "        \\verb|newspaper| & -0.001 & 0.0059 & -0.18 & 0.8599 \\\\ \\hline\n",
    "    \\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since we have multiple $p$-values, then we have multiple hypotheses that are being tested. Before we can discuss the hypotheses, we first describe the true model that we are trying to predict. Let $X_1$, $X_2$, and $X_3$ represent the random variables of  `TV`, `radio`, and `newspaper`, respectively, and let $Y$ represent the random variable of sales. We are therefore considering the linear regression model regressing `sales` onto the other three variables given by\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Table 3.4 then represents the output of fitting the following model using ordinary least squares (OLS):\n",
    "\n",
    "$$\n",
    "    \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3\n",
    "$$\n",
    "\n",
    "Thus, the null hypotheses for each variable is as follows: for each variable $X_i$ given in equation $(1)$, we have that $H_0$ is that the coefficient $\\beta_i = 0$, and the alternative hypotheseis $H_1$ is that the coefficient $\\beta_i \\neq 0$. Note that if the $p$-value is not less than 0.05, then we cannot reject $H_0$. This does not mean that $H_0$ is correct, but rather it means that the current sample data does not support the claim that $H_0$ is incorrect. In terms of these $p$-values, we can see that the only $p$-value not less than 0.05 is for coefficient $\\beta_3$ for `newspaper`. This means that we cannot reject the claim that the impact of `newspaper` on `sales` when considering the linear model given by $(1)$ is none. Thus, the current data does not suggest that advertising in newspaper along with TV and radio has any effect on the output of sales in either direction (positive or negative). While this does not prove that newspaper advertising is inefficient, it does suggest that newspaper advertising (assuming a linear model) is not effective in the light of both TV and radio advertising. However, we can definitely conclude that advertising for TV and radio is impactful in a positive way on sales. We do not see evidence that there is a relationship between `newspaper` and `sales`, but there is evidence of a relationship among `TV` and `radio` with `sales`.\n",
    "\n",
    "Finally, since the $p$-value associated with the intercept term $\\beta_0$ is less than 0.05, we do see that we can support the claim that $\\beta_0$ is non-zero. From the estimated value of $\\beta_0$, we see that in the absence of TV, radio, and newspaper advertising we still see positive sales values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Carefully explain the differences between the KNN classifier and KNN regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The largest difference between the KNN classifier and KNN regression model is the type of variable each predictive model is attempting to predict. For the KNN classifier, the predicted variable $Y$ is a categorical variable, meaning it takes on one of unique $n$ variables where $n \\in \\mathbb{N}$. For the KNN regression model, the predicted variable $Y$ is a continuous quantitaive variable. However, the general approach to each model is the same: consider the $K$ nearest neighbors to a value $x$ (or vector $\\mathbf{x}$ in the multiple classification/regression case). Moreover, we have the following functions $f(x_0)$ for predicting the value of $y \\in Y$ in the case of KNN regression:\n",
    "\n",
    "$$\n",
    "    \\bar{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_0}y_i,\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}_0$ is the $K$ nearest observations to $x_0$ and $y_i$ is the true value of $f(x_i) + \\epsilon$. In comparison, for KNN classification we assign to $x_0$ the class $Y = j$ which has the highest proportion of $K$ nearest neighbors within that class, i.e., we maximize the probability function\n",
    "\n",
    "$$\n",
    "    \\text{Pr}(Y = j | X = x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} I(y_i = j),\n",
    "$$\n",
    "\n",
    "where $I$ is the indicator function for observations $i \\in \\mathcal{N}_0$\n",
    "\n",
    "$$\n",
    "    I(y_i = j) = \\begin{cases} \n",
    "        1 & \\text{if $i$ belongs to class $j$,} \\\\\n",
    "        0 & \\text{if $i$ does not belong to class $j$.}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset with five predictors, $X_1 = \\text{GPA}$, $X_2 = \\text{IQ}$, $X_3 = \\text{Level (1 for College and 0 for High School)}$, $X_4 = \\text{Interaction between GPA and IQ}$, and $X_5 = \\text{Interaction between GPA and Level}$. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get that $\\hat{\\beta}_0 = 59$, ${\\beta}_1 = 20$, ${\\beta}_2 = 0.07$, ${\\beta}_3 = 35$, ${\\beta}_4 = 0.01$, and ${\\beta}_5 = -10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Which answer is correct, and why?\n",
    "\n",
    "- For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\n",
    "- For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\n",
    "- For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "- For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Note first that from the above numbers we have the following model:\n",
    "\n",
    "$$\n",
    "    \\hat{y} = 59 + 20(\\verb|GPA|) + 0.07(\\verb|IQ|) + 35 (\\verb|Level|) + 0.01(\\verb|GPA|)(\\verb|IQ|) - 10(\\verb|GPA|)(\\verb|Level|).\n",
    "$$\n",
    "\n",
    "We can quickly see that the third choice is correct, that for a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough. To see this, consider a GPA value of 3.5. Then a college graduate and a high school graduate with the same IQ will be predicted to make the same salary. However, as IQ is fixed and the GPA is increased, then the predicted value of salary will increase for the high school graduate will increase at a faster rate than that for the college gradaute. This is due to the term $- 10(\\verb|GPA|)(\\verb|Level|)$ which will be greater than $35 (\\verb|Level|)$, offsetting the gains of the college graduate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** From the propt, we can see that the value of $X_1$ is 4.0, $X_2$ is 110, and $X_3$ is 1. For $X_4$ and $X_5$ we can calculate the value using the above expression for $\\hat{y}$, giving us\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\hat{y} &= 59 + 20(4.0) + 0.07(110) + 35(1) + 0.01(4.0)(110) - 10 (4.0)(1), \\\\\n",
    "            &= 141.6.\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Thus we can approximate their salary to be $141,600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) True or false: Since the coefficient for GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** False, as it is not the magnitude of the coefficient of the GPA/IQ term (i.e., $\\hat{\\beta}_2$) that determines evidence of an interaction effect, but rather the resulting $p$-value from the hypothesis test that in the above model that $H_0: \\beta_2 = 0$ that determines if there is evidence of an interaction effect. As we are not given the $p$-value, we do not know if we can reject $H_0$ (and assume there is an interaction effect). However, note that even with a small magnitude of 0.07, this corresponds to $70, and with an IQ of 110 and GPA of 4.0 then we see that this adds an extra $30,800 to the predicted salary amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collect a set of data ($n = 100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Suppose that the true relationship between $X$ and $Y$ is linear, i.e. $Y = \\beta_0 + \\beta_1 X + \\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We would expect the RSS for the cubic regression to be lower than that of the linear regression. The reason is that regardless of the true nature of the model, due to the nature of the training RSS, for OLS models the RSS decreases monotonically as more predictors are added to the model. While the mathematical proof of this is beyond the scope of this model, the simple explanation is that the cubic model is more flexible than the linear model. As such, it is more able to adapt to the noise in the data ($\\epsilon$) and reduce the residuals, hense reducing the RSS value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Answer (a) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We would expect the test RSS for the cubic regression to be greater than that for the linear regression. The reason is that as the training RSS is derived from the training data, the test RSS is derived from unseen data and will generally reflect the true nature of the model. As the true nature of the model is linear, the estimated cubic model will vary greater from the true model than the linear model, resulting in a greater training RSS value.\n",
    "\n",
    "This is also supported by the bias-variance tradeoff, which is directly related to the test RSS. As the test RSS is dependent upon the model variance, squared bias, and error term, considering the two models we see that the error term will be the same, but the linear model will have greater bias and less variance while the cubic model will have greater variance and less bias. However, as the true model is linear, then the cubic model will be more susceptible to the training data that is used to train the model, resulting in a greater variance which will increase at a faster rate than the bias will decrease, resulting in an overall poorer fit with the test data and a higher RSS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Suppose that the true relationship between $X$ and $Y$ is non-linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Similarly to the answer in part (a), we would expect the training RSS for the cubic regression to be lower and the training RSS for the linear regression to be higher. This again is dependent not on the true nature of the model, but rather on the flexibiliy of the model that is being fit to the data. In this case, the cubic model is more flexible with more predictors, which will always result in an equivalent or lower RSS value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Generally speaking, as the true model is non-linear and the cubic model is more flexible, then we might expect that the test RSS value for the cubic model will be lower than that for the linear model following the bias-variance tradeoff as the cubic model will have more flexibility (i.e., less bias) than the linear model. However, as we do not know the true relationship, then it is impossible for us to estimate the variance term of either the linear or cubic model. While we know that the variance term of the linear model will be high as the true model is not linear, we do not know the full magnitude of the variance for the cubic model (as we do not know the true relationship). As such, it is impossible for us to definitely say that the test RSS for the cubic model will be lower than the test RSS of the linear model. There is not enough information to tell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i$-th fitted value take the form\n",
    "\n",
    "$$ \\hat{y}_i = x_i \\hat{\\beta}, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$  \n",
    "    \\hat{\\beta} = \\left(\\sum_{i=1}^n x_i y_i \\right) / \\left(\\sum_{i'=1}^n x_{i'}^2\\right).\n",
    "$$\n",
    "\n",
    "Show that we can write \n",
    "\n",
    "$$\n",
    "    \\hat{y}_i = \\sum_{i'=1}^n a_{i'}y_{i'}.\n",
    "$$\n",
    "\n",
    "What is $a_{i'}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Note that using the above formulas, we can write:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}  \n",
    "        \\hat{y}_i &= x_i \\hat{\\beta}_i \\\\\n",
    "            &= x_i \\left(\\sum_{j=1}^n x_j y_j \\right) / \\left(\\sum_{k=1}^n x_{k}^2\\right) \\\\\n",
    "            &= \\left(\\sum_{j=1}^n (x_j y_j)x_i \\right) / \\left(\\sum_{k=1}^n x_{k}^2\\right) \\\\\n",
    "            &= \\frac{(x_1y_1)x_i + (x_2y_2)x_i + \\cdots + (x_ny_n)x_i}{x_1^2 + x_2^2 + \\cdots + x_n^2} \\\\\n",
    "            &= \\left(\\frac{x_1x_i}{\\sum_{k=1}^n x_k^2}\\right)y_1 + \\left(\\frac{x_2x_i}{\\sum_{k=1}^n x_k^2}\\right)y_2 + \\cdots + \\left(\\frac{x_nx_i}{\\sum_{k=1}^n x_k^2}\\right)y_n \\\\\n",
    "            &= \\sum_{i'=1}^n a_{i'} y_{i'} \\hspace{.5cm} \\text{where} \\hspace{.5cm} a_{i'} = \\frac{x_{i'}x_i}{\\sum_{k=1}^n x_k^2}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Note that this means we can interpret this results as saying that the fitted values from linear regression (without an intercept) are linear combinations of the response values ($y_i$). In other words, each fitted value is a weighted average of the actual response values, where the weights are determined by the regression coefficient $\\hat{\\beta}$. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $(3.4)$, argue that in the case of simple linear regression, the least squares line always passes through the point $(\\bar{x}, \\bar{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** First note that equation $(3.4)$ is:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\\\\n",
    "        \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "We then see that showing the least squares line always passes through the point $(\\bar{x}, \\bar{y})$ is equivalent to showing that for the simple linear regression model\n",
    "\n",
    "$$ Y = f(X) + \\epsilon = \\beta_0 + \\beta_1 X + \\epsilon,$$\n",
    "\n",
    "we have for our predicted model\n",
    "\n",
    "$$ \\hat{y_i} = \\hat{f}(x_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$$\n",
    "\n",
    "that $\\hat{f}(\\bar{x}) = \\bar{y}$. We quickly see that is the case using the above expressions as\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\hat{f}(\\bar{x}) &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}, \\\\\n",
    "            &= (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 \\bar{x}, \\\\\n",
    "            &= \\bar{y}.\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Thus the least sqaures line always passes through the point $(\\bar{x}, \\bar{y})$. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is claimed that in the case of simple linear regression of $Y$ onto $X$, the $\\text{R}^2$ statistic (3.17) is equal to the square of the correlation between $X$ and $Y$ (3.18). Prove that this is the case. For simplicity, you may assume that $\\bar{x} = \\bar{y} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** First, note that from our previous equations we have that \n",
    "\n",
    "$$ \\text{Cor}(X,Y) = \\frac{\\sum_{i=1}^n (x_i -\\bar{x})(y_i -\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}. $$\n",
    "\n",
    "Assuming that $\\bar{x} = \\bar{y} = 0$, we can see that\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\left(\\text{Cor}(X,Y)\\right)^2 &= \\frac{\\left(\\sum_{i=1}^n x_iy_i\\right)^2}{\\left(\\sum_{i=1}^n x_i^2\\right)\\left(\\sum_{i=1}^n y_i^2\\right)}.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Thus this suffices to show that $\\text{R}^2$ is equal to equation $(1)$. Recall the we have for $R^2$ that \n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\text{R}^2 &= \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} \\\\\n",
    "        \\text{RSS} &= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n",
    "        \\text{TSS} &= \\sum_{i=1}^n (y_i - \\bar{y})^2 \\\\\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "As $\\bar{x} = \\bar{y} = 0$, we have that \n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\text{R}^2 = \\frac{\\sum_{i=1}^n y_i^2 - \\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n y_i^2}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Note that as $\\bar{x} = \\bar{y} = 0$, then we see that $\\hat{\\beta}_0 = 0$ and $\\hat{\\beta}_1 = (\\sum_{i=1}^n x_i y_i) / (\\sum_{i=1}^n x_i^2)$. Recall that $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i = \\hat{\\beta}_1 x_i$. Therefore we have that \n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\text{R}^2 &= \\frac{\\sum_{i=1}^n y_i^2 - \\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\sum_{i=1}^n y_i^2 - \\sum_{i=1}^n\\left[ y_i^2 - 2y_i\\hat{y}_i + \\hat{y}_i^2 \\right]}{\\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\sum_{i=1}^n 2y_i\\hat{y}_i - \\hat{y}_i^2}{\\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\sum_{i=1}^n 2y_i(\\hat{\\beta_1}x_i) - (\\hat{\\beta}_1 x_i)^2}{\\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\sum_{i=1}^n \\left[ 2y_i\\left(\\frac{\\sum_{i=1}^n x_iy_i}{\\sum_{i=1}^n x_i^2}x_i\\right) - \\left(\\frac{\\sum_{i=1}^n x_iy_i}{\\sum_{i=1}^n x_i^2} \\right)^2 x_i^2 \\right]}{\\sum_{i=1}^n y_i^2} \\cdot \\frac{\\sum_{i=1}^n x_i^2}{\\sum_{i=1}^n x_i^2} \\\\\n",
    "            &= \\frac{\\sum_{i=1}^n \\left[ 2x_iy_i \\left(\\sum_{i=1}^n x_iy_i\\right) - \\frac{\\left(\\sum_{i=1}^n x_iy_i\\right)^2}{\\sum_{i=1}^n x_i^2} x_i^2 \\right]}{\\sum_{i=1}^n x_i^2 \\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\left(\\sum_{i=1}^n x_iy_i\\right)\\sum_{i=1}^n \\left[ 2x_iy_i  - \\frac{\\sum_{i=1}^n x_iy_i}{\\sum_{i=1}^n x_i^2} x_i^2 \\right]}{\\sum_{i=1}^n x_i^2 \\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\left(\\sum_{i=1}^n x_iy_i\\right) \\left(\\sum_{i=1}^n 2x_iy_i  - \\sum_{i=1}^n x_i y_i \\left( \\sum_{i=1}^n \\frac{x_i^2}{\\sum_{i=1}^n x_i^2}\\right) \\right) }{\\sum_{i=1}^n x_i^2 \\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\left(\\sum_{i=1}^n x_iy_i\\right) \\left(\\sum_{i=1}^n 2x_iy_i  - x_i y_i \\right) }{\\sum_{i=1}^n x_i^2 \\sum_{i=1}^n y_i^2} && \\text{ as } \\left( \\sum_{i=1}^n \\frac{x_i^2}{\\sum_{i=1}^n x_i^2}\\right) = 1 \\\\\n",
    "            &= \\frac{\\left(\\sum_{i=1}^n x_iy_i\\right) \\left(\\sum_{i=1}^n x_i y_i \\right) }{\\sum_{i=1}^n x_i^2 \\sum_{i=1}^n y_i^2} \\\\\n",
    "            &= \\frac{\\left(\\sum_{i=1}^n x_iy_i\\right)^2}{\\left(\\sum_{i=1}^n x_i^2\\right)\\left(\\sum_{i=1}^n y_i^2\\right)}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Thus we see that in the case of simple linear regression we have that $\\text{R}^2 = (\\text{Cor}(X,Y))^2$. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question involve the use of simple linear regression on the `Auto` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Use the `sm.OLS()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Use the `summarize()` function to print the results. Comment on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "- Is there a relationship between the predictor and the response?\n",
    "- How strong is the relationship between the predictor and the response?\n",
    "- Is the relationship between the predictor and the response positive or negative?\n",
    "- What is the predited `mpg` associated with a `horsepower` of $98$? What are the associated $95\\%$ confidence and preiction intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** First we need to load in the `Auto` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chevrolet chevelle malibu</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buick skylark 320</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plymouth satellite</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amc rebel sst</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford torino</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mpg  cylinders  ...  year  origin\n",
       "name                                        ...              \n",
       "chevrolet chevelle malibu  18.0          8  ...    70       1\n",
       "buick skylark 320          15.0          8  ...    70       1\n",
       "plymouth satellite         18.0          8  ...    70       1\n",
       "amc rebel sst              16.0          8  ...    70       1\n",
       "ford torino                17.0          8  ...    70       1\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ISLP import load_data\n",
    "Auto = load_data(\"Auto\")\n",
    "\n",
    "display(Auto.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to extract the response column and predictor column for our model and then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.606\n",
      "Model:                            OLS   Adj. R-squared:                  0.605\n",
      "Method:                 Least Squares   F-statistic:                     599.7\n",
      "Date:                Mon, 14 Apr 2025   Prob (F-statistic):           7.03e-81\n",
      "Time:                        13:49:14   Log-Likelihood:                -1178.7\n",
      "No. Observations:                 392   AIC:                             2361.\n",
      "Df Residuals:                     390   BIC:                             2369.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         39.9359      0.717     55.660      0.000      38.525      41.347\n",
      "horsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n",
      "==============================================================================\n",
      "Omnibus:                       16.432   Durbin-Watson:                   0.920\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\n",
      "Skew:                           0.492   Prob(JB):                     0.000175\n",
      "Kurtosis:                       3.299   Cond. No.                         322.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract the intercept and horseposer variables \n",
    "X = pd.DataFrame({'horsepower': Auto['horsepower']})\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = Auto['mpg']         # Extract the response\n",
    "model = sm.OLS(y, X)    # Initiate the model\n",
    "results = model.fit()   # Fit the model\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we answer the proposed questions. Note that we are working with the following model:\n",
    "\n",
    "$$ \\verb|mpg| = \\beta_0 + \\beta_1 \\times \\verb|horsepower| $$\n",
    "\n",
    "1. Is there a relationship among the predictor and response?\n",
    "\n",
    "    Since this is simple linear regression with only one predictor variable, we merely need to check the results of the hypothesis test $H_0: \\beta_1 = 0$ (and the alternative hypothesis $H_1: \\beta_1 \\neq 0$). From the above summary, we can see that our corresponding $p$-value for the coefficient on `horsepower` is less than 0.05, meaning that the result is statistically significant. This supports the claim that there is a relationship among the predictor and response.\n",
    "\n",
    "2. How strong is the relationship between the predictor and the response?\n",
    "\n",
    "    We can review two measures of model accuracy to determine how strong the relationship is between the predictor and response. First, we use the RSE (residual standard error) to estimate the standard deviation of $\\epsilon$, the error term in our simple linear model. Note that the formula for RSE is\n",
    "\n",
    "    $$ \\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n - p - 1}} = \\sqrt{\\frac{1}{n - 2}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}.  $$\n",
    "\n",
    "    While we can compute this directly, we can also access the MSE (mean squared error) as $\\text{MSE} = \\text{RSE}^2$ through the `results` (see below). Reviewing the results, we see that the RSE is 4.9 unites and the mean value of the response `mpg` is 23.45, giving us a percentage error (or relative standard error) of roughly 20%. While this is higher than the desired 10%, it is still acceptable. We can also use $R^2$ to measure the strength of the relationship. From our above summary, the $R^2 = .6$, meaning that only 60% of the variance in `mpg` is explained by `horsepower`. Ideally we would like a higher $R^2$, indicating that more variace can be explained with additional predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSE: 4.91, y_bar: 23.45, percent error: 20.92%\n"
     ]
    }
   ],
   "source": [
    "# Calucate the RSE\n",
    "rse = np.sqrt(results.scale)\n",
    "\n",
    "# Calculate the mean value for the response\n",
    "y_bar = y.mean()\n",
    "\n",
    "print(f\"RSE: {round(rse,2)}, y_bar: {round(y_bar, 2)}, percent error: {round(rse/y_bar * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Is the relationship between the predictor and the response positive or negative?\n",
    "\n",
    "    Reviewing the above summary, we see that $\\hat{\\beta} = -0.1578$, meaning that an increase of 10 `horsepower` results in a reduction of the predicted `mpg` by 1. This means that increasing `horsepower` by 100 will reduce the average `mpg` by 10 miles per gallon, indicating a strong negative relationship between the predictor and response. Our predicted intercept is $\\hat{\\beta}_0 \\approx 40$, so with an average `horsepower` range of 100 - 200 can see a predicted `mpg` range of 20 - 30. Thus the relationship is negative, as an increase in `horsepower` will result in a decrease in `mpg`.\n",
    "\n",
    "4. What is the predited `mpg` associated with a `horsepower` of $98$? What are the associated $95\\%$ confidence and prediction intervals?\n",
    "\n",
    "    We can evaluate our model with a value of $98$ for `horsepower` and see that the predicted `mpg` is 24.46. We can also see that he 95% confidence interval (i.e., the true value of $f(98)$) is $(23.97, 24.96)$ but the 95% prediction interval (i.e., the true value of $y = f(98) + \\epsilon$) is $(14.81, 34.12)$ which is significantly greater than the confidence interval (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean_se</th>\n",
       "      <th>mean_ci_lower</th>\n",
       "      <th>mean_ci_upper</th>\n",
       "      <th>obs_ci_lower</th>\n",
       "      <th>obs_ci_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.467077</td>\n",
       "      <td>0.251262</td>\n",
       "      <td>23.973079</td>\n",
       "      <td>24.961075</td>\n",
       "      <td>14.809396</td>\n",
       "      <td>34.124758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean   mean_se  ...  obs_ci_lower  obs_ci_upper\n",
       "0  24.467077  0.251262  ...     14.809396     34.124758\n",
       "\n",
       "[1 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# New data for prediction\n",
    "new_X = np.array([1, 98])\n",
    "\n",
    "# Predict new value and get confidence intervals\n",
    "display(results.get_prediction(new_X).summary_frame(alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
