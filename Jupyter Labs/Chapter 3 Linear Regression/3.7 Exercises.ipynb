{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the null hypotheses to which the $p$-values given in Table 3.4 correspond. Explain what conclusions you can draw vased on these $p$-values. Your explanation should be phrased in terms of `sales`, `TV`, `radio`, and `newspaper`, rather than in terms of the coefficients of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Note Table 3.4: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\begin{array}{l|rrrr}\n",
    "        \\hline \n",
    "         & \\text{Coefficient} & \\text{Std. error} & t\\text{-statistic} & p\\text{-value} \\\\\n",
    "        \\hline\n",
    "        \\verb|Intercept| & 2.939 & 0.3119 & 9.42 & \\lt 0.0001 \\\\\n",
    "        \\verb|TV| & 0.046 & 0.0014 & 32.81 & \\lt 0.0001 \\\\\n",
    "        \\verb|radio| & 0.189 & 0.0086 & 21.89 & \\lt 0.0001 \\\\\n",
    "        \\verb|newspaper| & -0.001 & 0.0059 & -0.18 & 0.8599 \\\\ \\hline\n",
    "    \\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since we have multiple $p$-values, then we have multiple hypotheses that are being tested. Before we can discuss the hypotheses, we first describe the true model that we are trying to predict. Let $X_1$, $X_2$, and $X_3$ represent the random variables of  `TV`, `radio`, and `newspaper`, respectively, and let $Y$ represent the random variable of sales. We are therefore considering the linear regression model regressing `sales` onto the other three variables given by\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Table 3.4 then represents the output of fitting the following model using ordinary least squares (OLS):\n",
    "\n",
    "$$\n",
    "    \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3\n",
    "$$\n",
    "\n",
    "Thus, the null hypotheses for each variable is as follows: for each variable $X_i$ given in equation $(1)$, we have that $H_0$ is that the coefficient $\\beta_i = 0$, and the alternative hypotheseis $H_1$ is that the coefficient $\\beta_i \\neq 0$. Note that if the $p$-value is not less than 0.05, then we cannot reject $H_0$. This does not mean that $H_0$ is correct, but rather it means that the current sample data does not support the claim that $H_0$ is incorrect. In terms of these $p$-values, we can see that the only $p$-value not less than 0.05 is for coefficient $\\beta_3$ for `newspaper`. This means that we cannot reject the claim that the impact of `newspaper` on `sales` when considering the linear model given by $(1)$ is none. Thus, the current data does not suggest that advertising in newspaper along with TV and radio has any effect on the output of sales in either direction (positive or negative). While this does not prove that newspaper advertising is inefficient, it does suggest that newspaper advertising (assuming a linear model) is not effective in the light of both TV and radio advertising. However, we can definitely conclude that advertising for TV and radio is impactful in a positive way on sales. We do not see evidence that there is a relationship between `newspaper` and `sales`, but there is evidence of a relationship among `TV` and `radio` with `sales`.\n",
    "\n",
    "Finally, since the $p$-value associated with the intercept term $\\beta_0$ is less than 0.05, we do see that we can support the claim that $\\beta_0$ is non-zero. From the estimated value of $\\beta_0$, we see that in the absence of TV, radio, and newspaper advertising we still see positive sales values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Carefully explain the differences between the KNN classifier and KNN regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The largest difference between the KNN classifier and KNN regression model is the type of variable each predictive model is attempting to predict. For the KNN classifier, the predicted variable $Y$ is a categorical variable, meaning it takes on one of unique $n$ variables where $n \\in \\mathbb{N}$. For the KNN regression model, the predicted variable $Y$ is a continuous quantitaive variable. However, the general approach to each model is the same: consider the $K$ nearest neighbors to a value $x$ (or vector $\\mathbf{x}$ in the multiple classification/regression case). Moreover, we have the following functions $f(x_0)$ for predicting the value of $y \\in Y$ in the case of KNN regression:\n",
    "\n",
    "$$\n",
    "    \\bar{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_0}y_i,\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}_0$ is the $K$ nearest observations to $x_0$ and $y_i$ is the true value of $f(x_i) + \\epsilon$. In comparison, for KNN classification we assign to $x_0$ the class $Y = j$ which has the highest proportion of $K$ nearest neighbors within that class, i.e., we maximize the probability function\n",
    "\n",
    "$$\n",
    "    \\text{Pr}(Y = j | X = x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} I(y_i = j),\n",
    "$$\n",
    "\n",
    "where $I$ is the indicator function for observations $i \\in \\mathcal{N}_0$\n",
    "\n",
    "$$\n",
    "    I(y_i = j) = \\begin{cases} \n",
    "        1 & \\text{if $i$ belongs to class $j$,} \\\\\n",
    "        0 & \\text{if $i$ does not belong to class $j$.}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset with five predictors, $X_1 = \\text{GPA}$, $X_2 = \\text{IQ}$, $X_3 = \\text{Level (1 for College and 0 for High School)}$, $X_4 = \\text{Interaction between GPA and IQ}$, and $X_5 = \\text{Interaction between GPA and Level}$. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get that $\\hat{\\beta}_0 = 59$, ${\\beta}_1 = 20$, ${\\beta}_2 = 0.07$, ${\\beta}_3 = 35$, ${\\beta}_4 = 0.01$, and ${\\beta}_5 = -10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Which answer is correct, and why?\n",
    "\n",
    "- For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\n",
    "- For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\n",
    "- For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "- For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Note first that from the above numbers we have the following model:\n",
    "\n",
    "$$\n",
    "    \\hat{y} = 59 + 20(\\verb|GPA|) + 0.07(\\verb|IQ|) + 35 (\\verb|Level|) + 0.01(\\verb|GPA|)(\\verb|IQ|) - 10(\\verb|GPA|)(\\verb|Level|).\n",
    "$$\n",
    "\n",
    "We can quickly see that the third choice is correct, that for a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough. To see this, consider a GPA value of 3.5. Then a college graduate and a high school graduate with the same IQ will be predicted to make the same salary. However, as IQ is fixed and the GPA is increased, then the predicted value of salary will increase for the high school graduate will increase at a faster rate than that for the college gradaute. This is due to the term $- 10(\\verb|GPA|)(\\verb|Level|)$ which will be greater than $35 (\\verb|Level|)$, offsetting the gains of the college graduate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** From the propt, we can see that the value of $X_1$ is 4.0, $X_2$ is 110, and $X_3$ is 1. For $X_4$ and $X_5$ we can calculate the value using the above expression for $\\hat{y}$, giving us\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\hat{y} &= 59 + 20(4.0) + 0.07(110) + 35(1) + 0.01(4.0)(110) - 10 (4.0)(1), \\\\\n",
    "            &= 141.6.\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Thus we can approximate their salary to be $141,600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) True or false: Since the coefficient for GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** False, as it is not the magnitude of the coefficient of the GPA/IQ term (i.e., $\\hat{\\beta}_2$) that determines evidence of an interaction effect, but rather the resulting $p$-value from the hypothesis test that in the above model that $H_0: \\beta_2 = 0$ that determines if there is evidence of an interaction effect. As we are not given the $p$-value, we do not know if we can reject $H_0$ (and assume there is an interaction effect). However, note that even with a small magnitude of 0.07, this corresponds to $70, and with an IQ of 110 and GPA of 4.0 then we see that this adds an extra $30,800 to the predicted salary amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collect a set of data ($n = 100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Suppose that the true relationship between $X$ and $Y$ is linear, i.e. $Y = \\beta_0 + \\beta_1 X + \\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We would expect the RSS for the cubic regression to be lower than that of the linear regression. The reason is that regardless of the true nature of the model, due to the nature of the training RSS, for OLS models the RSS decreases monotonically as more predictors are added to the model. While the mathematical proof of this is beyond the scope of this model, the simple explanation is that the cubic model is more flexible than the linear model. As such, it is more able to adapt to the noise in the data ($\\epsilon$) and reduce the residuals, hense reducing the RSS value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Answer (a) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We would expect the test RSS for the cubic regression to be greater than that for the linear regression. The reason is that as the training RSS is derived from the training data, the test RSS is derived from unseen data and will generally reflect the true nature of the model. As the true nature of the model is linear, the estimated cubic model will vary greater from the true model than the linear model, resulting in a greater training RSS value.\n",
    "\n",
    "This is also supported by the bias-variance tradeoff, which is directly related to the test RSS. As the test RSS is dependent upon the model variance, squared bias, and error term, considering the two models we see that the error term will be the same, but the linear model will have greater bias and less variance while the cubic model will have greater variance and less bias. However, as the true model is linear, then the cubic model will be more susceptible to the training data that is used to train the model, resulting in a greater variance which will increase at a faster rate than the bias will decrease, resulting in an overall poorer fit with the test data and a higher RSS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Suppose that the true relationship between $X$ and $Y$ is non-linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Similarly to the answer in part (a), we would expect the training RSS for the cubic regression to be lower and the training RSS for the linear regression to be higher. This again is dependent not on the true nature of the model, but rather on the flexibiliy of the model that is being fit to the data. In this case, the cubic model is more flexible with more predictors, which will always result in an equivalent or lower RSS value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Generally speaking, as the true model is non-linear and the cubic model is more flexible, then we might expect that the test RSS value for the cubic model will be lower than that for the linear model following the bias-variance tradeoff as the cubic model will have more flexibility (i.e., less bias) than the linear model. However, as we do not know the true relationship, then it is impossible for us to estimate the variance term of either the linear or cubic model. While we know that the variance term of the linear model will be high as the true model is not linear, we do not know the full magnitude of the variance for the cubic model (as we do not know the true relationship). As such, it is impossible for us to definitely say that the test RSS for the cubic model will be lower than the test RSS of the linear model. There is not enough information to tell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
